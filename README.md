# EWT-Seq2Seq-LSTM-Attention
EWT-Seq2Seq-LSTM-Attention

This repository presents a an Optimized EWT-Seq2Seq-LSTM with Attention Mechanism.

Two time-varying sequences are generated to evaluate a Sequence-to-sequence learning (Seq2Seq) signal.


Wrote by Dr. Laio Oriel Seman and Dr. Stefano Frizzo Stefenon.

Trento, Italy, March 03, 2023.

The proposed analysis is divided according to the models evaluated:

Seq2Seq-LSTM with Attention: Evaluation of the starting model using the Attention Mechanism.

Optimized Seq2Seq-LSTM with Attention: Evaluation of the model with the optimized hyperparameters (based on Optuna).

Standard Seq2Seq LSTM: Standard model for comparative purposes.
